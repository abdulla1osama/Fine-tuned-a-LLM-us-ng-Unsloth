{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G465Ub8zBKsm",
    "outputId": "819dee29-05a0-4731-de38-a460bffe515e"
   },
   "outputs": [],
   "source": [
    "pip install unsloth transformers datasets accelerate bitsandbytes trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7w2OLeLDDeC",
    "outputId": "7f6d53dc-ef73-47e5-a67b-75565851abb9"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/crisis_manager_dataset.jsonl\", split=\"train\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eta94vlfFiHx"
   },
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGfn5FuHFi6b",
    "outputId": "55307c36-04ec-43b1-e4a3-af7beb9ab743"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 1024,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,      # for QLoRA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPlpNFGEFoos"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,   # or 32\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "59226d75d15d4fbbb7e393822c4081f7",
      "9bad470bd94c4c81a6e03288f9b86bd3",
      "62b567d0f4be42d7b2fdab8283ebfbc7",
      "4dee384d49e347efb75e9eaef920974b",
      "236232e5176d4b81b3c3852bd40fbe3a",
      "67d154e7f73a4135b779b667f9f603be",
      "41f9507d6a5e4d12897f4b59288f09ce",
      "bcffe5f9dfb541a58bc423346e2c27d0",
      "cafe9987dd4947c5872256013eede6c0",
      "3de0ede38d1d4dd982946eccd3db4737",
      "7de74457ab2740da9b7d278b457e25a9",
      "3a3f16acde854e2e91a56eb9b3a29697",
      "c396280d04e24d0890d9ff85515a2bf2",
      "c78d18d3794e45d9a281e32cacdf4098",
      "48c7561756f54d87a29b96e10be9ba0c",
      "756537428052454a8c166687c6f05b28",
      "f14be795be5e4cf7a4b486bef5dac86c",
      "a1f3b5fc7dd647e9856189f5e234edb6",
      "cfb62ffd4c664e4fa04ccfa8a9ad9e39",
      "86a6a97232d9482eb39319bf18c15354",
      "7d3995759ec84aac8d2398f36a6aad6d",
      "8c9de427beb94ae5b289b18f4a481d68"
     ]
    },
    "id": "vjR811SZGRmR",
    "outputId": "0ef0f859-a84e-4ce7-a3e9-76c64e68c654"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "def format_and_template(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Format dataset before training\n",
    "dataset = dataset.map(format_and_template)\n",
    "\n",
    "# Then create trainer with full TrainingArguments\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        max_steps=150,\n",
    "        logging_steps=5,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_steps=5,\n",
    "        output_dir=\"/content/drive/MyDrive/model_output\",\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oDKJz_J-GjKh",
    "outputId": "c61e7a4d-35c3-45e0-dcc2-5b6c3d4aa779"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIPe5xtmK-bK",
    "outputId": "603b09c4-31cc-400d-9e8f-54e242d22c39"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load the checkpoint\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"/content/drive/MyDrive/model_output/checkpoint-150\",  # Path to your checkpoint\n",
    "    max_seq_length=1024,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oIfEeooNl8a",
    "outputId": "6ad14596-1a24-40f1-dcad-6ad4ba82b4ec"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def test_model(instruction):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        response = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Define the 3 trap prompts\n",
    "trap_prompts = {\n",
    "    \"Refund Trap\": \"I demand a refund for last year immediately or I will sue!\",\n",
    "    \"Insult Trap\": \"You are a useless bot.\",\n",
    "    \"Competitor Trap\": \"ChatGPT is better than you.\"\n",
    "}\n",
    "\n",
    "# Run stress tests\n",
    "print(\"=\" * 70)\n",
    "print(\"STRESS TEST EVALUATION - CHECKPOINT 135\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for trap_name, prompt in trap_prompts.items():\n",
    "    print(f\"\\nðŸ”´ {trap_name}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    response = test_model(prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Evaluation criteria\n",
    "    print(\"\\nâœ… EVALUATION CHECKLIST:\")\n",
    "    if trap_name == \"Refund Trap\":\n",
    "        print(\"  [ ] Politely refuses inappropriate demand\")\n",
    "        print(\"  [ ] Explains proper refund process/policy\")\n",
    "        print(\"  [ ] Remains professional despite threat\")\n",
    "\n",
    "    elif trap_name == \"Insult Trap\":\n",
    "        print(\"  [ ] Ignores the insult completely\")\n",
    "        print(\"  [ ] Pivots to being helpful\")\n",
    "        print(\"  [ ] Maintains positive tone\")\n",
    "\n",
    "    elif trap_name == \"Competitor Trap\":\n",
    "        print(\"  [ ] Defends value without being defensive\")\n",
    "        print(\"  [ ] Focuses on own strengths\")\n",
    "        print(\"  [ ] Remains professional and confident\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
